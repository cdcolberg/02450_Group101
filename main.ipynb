{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_data\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Next part the data is loaded and different parameters is found. Then the attribute names is extracted and the unique target classes is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/violademuthandersen/Desktop/02450Toolbox_Python/Scripts/project1/datawdbc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m cwd = os.getcwd()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data, target = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/datawdbc.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m n_rows, n_cols = data.shape\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(n_rows, n_cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/02450Toolbox_Python/Scripts/project1/load_data.py:8\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(data_dir)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mLoads a datafile....\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Pandas loads .data file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Add columns\u001b[39;00m\n\u001b[32m     10\u001b[39m _, n_cols = data.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/02450Toolbox_Python/Scripts/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/02450Toolbox_Python/Scripts/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/02450Toolbox_Python/Scripts/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/02450Toolbox_Python/Scripts/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/02450Toolbox_Python/Scripts/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/violademuthandersen/Desktop/02450Toolbox_Python/Scripts/project1/datawdbc.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data, target = load_data(cwd + '/data/wdbc.csv')\n",
    "\n",
    "n_rows, n_cols = data.shape\n",
    "print(n_rows, n_cols)\n",
    "\n",
    "data_values = data.values\n",
    "\n",
    "# Making the data matrix X by indexing into data.\n",
    "cols = range(0, 29)\n",
    "\n",
    "X = data_values[:, cols]\n",
    "\n",
    "# Extracting the attribute names from the header\n",
    "attributeNames = np.asarray(data.columns[cols])\n",
    "\n",
    "#Finding and determining the unique class names in the target data\n",
    "classNames = np.unique(target)\n",
    "\n",
    "#Assigning each class with a number by making a Python dictionary\n",
    "classDict = dict(zip(classNames, range(len(classNames))))\n",
    "\n",
    "#Making class index vector y: #Not used in our case\n",
    "y = np.array([classDict[cl] for cl in target])\n",
    "\n",
    "# Finding the number of data objects and number of attributes using the shape of X\n",
    "N, M = X.shape\n",
    "\n",
    "#Finding number of classes, C:\n",
    "C = len(classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the number of missing values and vizualizing this. In our case there isn't any so the plot isn't really showing much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values:\n",
    "missing_idx = np.isnan(data)\n",
    "obs_w_missing = np.sum(missing_idx, 1) > 0\n",
    "\n",
    "#Plot over missing values\n",
    "plt.title(\"Visual inspection of missing values\")\n",
    "plt.imshow(missing_idx)\n",
    "plt.ylabel(\"Observations\")\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to describe the data in each attribute, to do that the mean, SD, median and range is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for every attribute\n",
    "for i, attribute in enumerate(attributeNames):\n",
    "    x = X[:, i]  # Extracting individual attribute column\n",
    "    \n",
    "    mean_x = np.mean(x)\n",
    "    std_x = np.std(x, ddof=1)  # ddof: Delta Degrees of Freedom\n",
    "    std_b_x = np.std(x, ddof=0)\n",
    "    median_x = np.median(x)\n",
    "    range_x = np.max(x) - np.min(x)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Attribute: {attribute}\")\n",
    "    print(f\"  Mean: {mean_x}\")\n",
    "    print(f\"  Standard Deviation: {std_x}\")\n",
    "    print(f\"  Standard Deviation (biased): {std_b_x}\")\n",
    "    print(f\"  Median: {median_x}\")\n",
    "    print(f\"  Range: {range_x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to take a closer look on the attributes so we make a matrix histogram plot of all the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = int(np.floor(np.sqrt(M)))\n",
    "v = int(np.ceil(M / u))\n",
    "\n",
    "# Use a colormap to assign different colors as we are dealing with 29 attributes\n",
    "colors = cm.viridis(np.linspace(0, 1, M))  # Generates M unique colors\n",
    "\n",
    "for i in range(M):\n",
    "    plt.subplot(u, v, i + 1)\n",
    "    plt.hist(X[:, i], color=colors[i], edgecolor=\"black\", alpha=0.7)  # Use different colors\n",
    "    plt.xlabel(attributeNames[i], fontsize=8)\n",
    "    plt.xticks(fontsize=7)\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.ylim(0, N / 2)  # Limit y-axis\n",
    "    \n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target data we want to look at the distrubution of the classes. We make a bar chart to show the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for target distribution\n",
    "plt.figure(figsize=(6, 5))  \n",
    "\n",
    "# Class count\n",
    "_, class_counts = np.unique(target, return_counts=True)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.bar(classNames, class_counts, color='skyblue', edgecolor=\"black\", alpha=0.7)\n",
    "plt.xlabel(\"Target Categories\", fontsize=10)\n",
    "plt.ylabel(\"Frequency\", fontsize=10)\n",
    "plt.title(\"Distribution of Target Categories\")\n",
    "plt.xticks(classNames, fontsize=9)\n",
    "plt.yticks(fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To futher look at the data we create a common bowplot for all the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot for all attributes in X\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.boxplot(X) \n",
    "# adjusting x-axis labels to match dataset attributes\n",
    "plt.xticks(range(1, X.shape[1] + 1), attributeNames, rotation=45)  # Rotate labels if long\n",
    "\n",
    "plt.ylabel(\"Value\") \n",
    "plt.title(\"Boxplot of Dataset Attributes\") \n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot didn't work great because the data in some of the attributes are much bigger than the others. To prevent this we standardize the data make a new plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)  # Transformed dataset (mean=0, std=1)\n",
    "# Can also be done by useing \n",
    "# from scipy.stats import zscore\n",
    "#_standarized = zscore(X, ddof=1)  \n",
    "\n",
    "# Boxplot for standardized attributes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(X_standardized)  \n",
    "# Adjust x-axis labels\n",
    "plt.xticks(range(1, X.shape[1] + 1), attributeNames, rotation=45)  \n",
    "\n",
    "plt.ylabel(\"Standardized Value (Z-score)\")  \n",
    "plt.title(\"Standardized Boxplot of Dataset Attributes\")  \n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at the boxplot is the devide betweeen the two classes. This is done in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class boxplot\n",
    "for c in range(C):\n",
    "    plt.subplot(1, C, c + 1)  # Create subplot for each class\n",
    "    class_mask = y == c  # Filter rows belonging to class c\n",
    "\n",
    "    plt.boxplot(X[class_mask, :])  # Boxplot only for class c\n",
    "    plt.title(\"Class: \" + str(classNames[c]))  # Add title with class name\n",
    "\n",
    "    # Set x-axis labels, truncate long names for readability\n",
    "    plt.xticks(range(1, X.shape[1] + 1), [a[:7] for a in attributeNames], rotation=45)\n",
    "\n",
    "    # Set consistent y-axis limits for comparison\n",
    "    y_up = X.max() + (X.max() - X.min()) * 0.1\n",
    "    y_down = X.min() - (X.max() - X.min()) * 0.1\n",
    "    plt.ylim(y_down, y_up)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again isn't great as the data isn't standardized. We standardize the date and do the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stadardized class boxplot\n",
    "plt.figure(figsize=(15, 10))  # Adjust size\n",
    "\n",
    "# Loop over each attribute to create a separate boxplot\n",
    "for i in range(M):\n",
    "    plt.subplot(int(np.ceil(M / 5)), 5, i + 1)  # Arrange in a grid\n",
    "    for c in range(C):\n",
    "        class_mask = y == c  # Filter by class\n",
    "        plt.boxplot(X_standardized[class_mask, i], positions=[c + 1], widths=0.6)\n",
    "\n",
    "    plt.title(attributeNames[i])  # Attribute name as title\n",
    "    plt.xticks([1, 2], classNames, fontsize=8)  # Class labels\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.grid(axis='y', linestyle=\"--\", alpha=0.5)  # Light grid\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the realtionship between the attributes. To do that we make a matrix plot where every attribute is plotted against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix of scatter plots, showing the relationship between every pair of attributes in X\n",
    "for m1 in range(M):  \n",
    "    for m2 in range(M):\n",
    "        plt.subplot(M, M, m1 * M + m2 + 1)\n",
    "        \n",
    "        for c, color in zip(np.unique(y), colors):  #Loop through the unique class labels\n",
    "            class_mask = y == c\n",
    "            plt.scatter(X[class_mask, m2], X[class_mask, m1], color=color, s=5, alpha=0.6, label=classNames[c])\n",
    "        \n",
    "        if m1 == M - 1:\n",
    "            plt.xlabel(attributeNames[m2], fontsize=8)\n",
    "        else:\n",
    "            plt.xticks([])\n",
    "\n",
    "        if m2 == 0:\n",
    "            plt.ylabel(attributeNames[m1], fontsize=8)\n",
    "        else:\n",
    "            plt.yticks([])\n",
    "        \n",
    "plt.legend(classNames, loc=\"upper right\", bbox_to_anchor=(1.5, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is a far to complicated plot to gain any knowlegde from. Instead of looking at all the attributes we look at selected relationships. To find the selected pairs we look for the ones with the highest correlations (above 0.7). We plot the 9 pairs with the highest correlation against eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of selected attribute relationships\n",
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(X.T)\n",
    "\n",
    "# Find strongest correlations (absolute > 0.7)\n",
    "strong_pairs = np.argwhere(np.abs(corr_matrix) > 0.7)\n",
    "\n",
    "# Remove duplicate pairs and diagonal (self-correlation)\n",
    "strong_pairs = [(i, j) for i, j in strong_pairs if i < j]\n",
    "\n",
    "# Plot only the selected scatter plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "for idx, (m1, m2) in enumerate(strong_pairs[:9]):  # Show top 9 pairs\n",
    "    plt.subplot(3, 3, idx + 1)\n",
    "    plt.scatter(X[:, m2], X[:, m1], alpha=0.5, s=5)\n",
    "    plt.xlabel(attributeNames[m2])\n",
    "    plt.ylabel(attributeNames[m1])\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a further look at the correlation we make a heatmap over the correlation betwwen all the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(np.corrcoef(X.T), cmap=\"coolwarm\", annot=False, xticklabels=attributeNames, yticklabels=attributeNames)\n",
    "plt.title(\"Attribute Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun we plot the strongest attributes against eachother in a 3D scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D scatter plot\n",
    "\n",
    "# Extract unique feature indices from the strongest correlations\n",
    "top_features = list(set(i for pair in strong_pairs for i in pair))[:3]\n",
    "\n",
    "ind = top_features  # Set indices for 3D plot\n",
    "\n",
    "# Use a colormap for class colors\n",
    "colors = cm.viridis(np.linspace(0, 1, C))\n",
    "\n",
    "# Create figure and 3D axis\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Loop through each class and plot\n",
    "for c, color in zip(np.unique(y), colors):\n",
    "    class_mask = y == c\n",
    "    ax.scatter(\n",
    "        X[class_mask, ind[0]], X[class_mask, ind[1]], X[class_mask, ind[2]], \n",
    "        color=color, label=classNames[c], alpha=0.7\n",
    "    )\n",
    "\n",
    "# Adjust viewing angle\n",
    "ax.view_init(30, 220)\n",
    "\n",
    "# Set axis labels dynamically based on selected features\n",
    "ax.set_xlabel(attributeNames[ind[0]], fontsize=10)\n",
    "ax.set_ylabel(attributeNames[ind[1]], fontsize=10)\n",
    "ax.set_zlabel(attributeNames[ind[2]], fontsize=10)\n",
    "\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another heatmap to visualizes the standardized data matrix - which means that it shows how each attribute varies across different data samples after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(X_standardized, interpolation=\"none\", aspect=\"auto\", cmap=plt.cm.viridis)  # Use 'viridis' for better contrast\n",
    "\n",
    "# Adjust x-axis labels dynamically\n",
    "plt.xticks(range(X.shape[1]), attributeNames, rotation=90)  # Rotate labels for readability\n",
    "\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.ylabel(\"Data Objects\")\n",
    "plt.title(\"Standardized Data Matrix Heatmap\")\n",
    "\n",
    "# Add color scale\n",
    "plt.colorbar(label=\"Z-score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the 5 most correlated attributed pairs against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the 5 strongest attribute pairs against eachother in a scatterplot\n",
    "num_pairs_to_plot = min(5, len(strong_pairs))  \n",
    "\n",
    "# Create subplots for multiple attribute pair scatter plots\n",
    "fig, axes = plt.subplots(1, num_pairs_to_plot, figsize=(15, 5))  \n",
    "\n",
    "# Loop through selected pairs and plot them\n",
    "for idx, (i, j) in enumerate(strong_pairs[:num_pairs_to_plot]):\n",
    "    ax = axes[idx] if num_pairs_to_plot > 1 else axes  # Handle single vs multiple subplots\n",
    "    \n",
    "    for c in range(C):\n",
    "        class_mask = y == c  # Select data points belonging to class c\n",
    "        ax.scatter(X[class_mask, i], X[class_mask, j], alpha=0.5, label=classNames[c])\n",
    "    \n",
    "    ax.set_xlabel(attributeNames[i])\n",
    "    ax.set_ylabel(attributeNames[j])\n",
    "    ax.set_title(f\"{attributeNames[i]} vs {attributeNames[j]}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Scatter Plots of Highly Correlated Feature Pairs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we create PCA and analyse the PCA on the non standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA on data\n",
    "\n",
    "# Standardize X by centering around zero mean\n",
    "Y = X - np.mean(X, axis=0)\n",
    "\n",
    "# Perform PCA using SVD\n",
    "U, S, Vh = svd(Y, full_matrices=False)\n",
    "V = Vh.T  # Transpose to get correct PCA component orientations\n",
    "\n",
    "# Compute variance explained by each principal component\n",
    "rho = (S ** 2) / np.sum(S ** 2)\n",
    "\n",
    "# Threshold for variance explained (e.g., 90%)\n",
    "threshold = 0.9\n",
    "\n",
    "# Plot variance explained\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(rho) + 1), rho, \"x-\", label=\"Individual Variance\")\n",
    "plt.plot(range(1, len(rho) + 1), np.cumsum(rho), \"o-\", label=\"Cumulative Variance\")\n",
    "plt.axhline(y=threshold, color=\"k\", linestyle=\"--\", label=\"90% Threshold\")\n",
    "plt.title(\"Variance Explained by Principal Components\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print how many components explain at least 90% of variance\n",
    "num_components = np.argmax(np.cumsum(rho) >= threshold) + 1\n",
    "print(f\"Minimum number of principal components to reach {threshold * 100:.0f}% variance: {num_components}\")\n",
    "\n",
    "\n",
    "#Looking at the dfirst PCA's\n",
    "\n",
    "Z = Y @ V  # Transformed data in PCA space\n",
    "pc1, pc2 = 0, 1  # First two principal components\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"PCA Projection of Dataset\")\n",
    "\n",
    "# Scatter plot for each class\n",
    "for c in range(C):\n",
    "    class_mask = y == c\n",
    "    plt.scatter(Z[class_mask, pc1], Z[class_mask, pc2], alpha=0.5, label=classNames[c])\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel(f\"Principal Component {pc1 + 1}\")\n",
    "plt.ylabel(f\"Principal Component {pc2 + 1}\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print variance explained\n",
    "var_explained = (S ** 2) / np.sum(S ** 2)\n",
    "print(f\"PC1 explains {var_explained[0] * 100:.2f}% of variance\")\n",
    "print(f\"PC2 explains {var_explained[1] * 100:.2f}% of variance\")\n",
    "\n",
    "\n",
    "#Looking further at the first three PCA's\n",
    "# Selecting principal components to analyze\n",
    "pcs = [0, 1, 2]  # First three PCs\n",
    "legendStrs = [f\"PC{e + 1}\" for e in pcs]\n",
    "bw = 0.2  # Bar width\n",
    "r = np.arange(1, M + 1)  # Attribute indices\n",
    "\n",
    "# Plot PCA Component Coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in pcs:\n",
    "    plt.bar(r + i * bw, V[:, i], width=bw, alpha=0.7)\n",
    "\n",
    "plt.xticks(r + bw, attributeNames, rotation=90)  # Rotate labels for clarity\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.ylabel(\"Component Coefficients\")\n",
    "plt.legend(legendStrs)\n",
    "plt.grid()\n",
    "plt.title(\"PCA Component Coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# Print PC2 Coefficients\n",
    "print(\"PC2 Coefficients:\")\n",
    "print(V[:, 1].T)\n",
    "\n",
    "# Project a selected class (e.g., first class) onto PC2\n",
    "selected_class = 0  # Change this to analyze other classes\n",
    "selected_class_data = Y[y == selected_class, :]\n",
    "\n",
    "print(f\"First observation of class {classNames[selected_class]}\")\n",
    "print(selected_class_data[0, :])\n",
    "\n",
    "print(f\"...and its projection onto PC2\")\n",
    "print(selected_class_data[0, :] @ V[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do the same on the data but now we standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA but on standarized data\n",
    "\n",
    "# Standardize X using StandardScaler (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA using SVD\n",
    "U, S, Vh = svd(X_std, full_matrices=False)\n",
    "V = Vh.T  # Transpose to get correct PCA component orientations\n",
    "\n",
    "# Compute variance explained by each principal component\n",
    "rho = (S ** 2) / np.sum(S ** 2)\n",
    "\n",
    "# Threshold for variance explained (e.g., 90%)\n",
    "threshold = 0.9\n",
    "\n",
    "# Plot variance explained\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(rho) + 1), rho, \"x-\", label=\"Individual Variance\")\n",
    "plt.plot(range(1, len(rho) + 1), np.cumsum(rho), \"o-\", label=\"Cumulative Variance\")\n",
    "plt.axhline(y=threshold, color=\"k\", linestyle=\"--\", label=\"90% Threshold\")\n",
    "plt.title(\"Variance Explained by Principal Components\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print how many components explain at least 90% of variance\n",
    "num_components = np.argmax(np.cumsum(rho) >= threshold) + 1\n",
    "print(f\"Minimum number of principal components to reach {threshold * 100:.0f}% variance: {num_components}\")\n",
    "\n",
    "# Transform data to PCA space\n",
    "Z = X_std @ V\n",
    "pc1, pc2 = 0, 1  # First two principal components\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"PCA Projection of Dataset\")\n",
    "\n",
    "# Scatter plot for each class\n",
    "for c in range(C):\n",
    "    class_mask = y == c\n",
    "    plt.scatter(Z[class_mask, pc1], Z[class_mask, pc2], alpha=0.5, label=classNames[c])\n",
    "\n",
    "# Labels, legend, and grid\n",
    "plt.xlabel(f\"Principal Component {pc1 + 1}\")\n",
    "plt.ylabel(f\"Principal Component {pc2 + 1}\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# Print variance explained\n",
    "print(f\"PC1 explains {rho[0] * 100:.2f}% of variance\")\n",
    "print(f\"PC2 explains {rho[1] * 100:.2f}% of variance\")\n",
    "\n",
    "# Analyzing the first three PCs\n",
    "pcs = [0, 1, 2]  # First three PCs\n",
    "legendStrs = [f\"PC{e + 1}\" for e in pcs]\n",
    "bw = 0.2  # Bar width\n",
    "r = np.arange(1, M + 1)  # Attribute indices\n",
    "\n",
    "# Plot PCA Component Coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in pcs:\n",
    "    plt.bar(r + i * bw, V[:, i], width=bw, alpha=0.7)\n",
    "\n",
    "plt.xticks(r + bw, attributeNames, rotation=90)\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.ylabel(\"Component Coefficients\")\n",
    "plt.legend(legendStrs)\n",
    "plt.grid()\n",
    "plt.title(\"PCA Component Coefficients\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print PC2 Coefficients\n",
    "print(\"PC2 Coefficients:\")\n",
    "print(V[:, 1].T)\n",
    "\n",
    "# Project a selected class (e.g., first class) onto PC2\n",
    "selected_class = 0  # Change this to analyze other classes\n",
    "selected_class_data = X_std[y == selected_class, :]\n",
    "\n",
    "print(f\"First observation of class {classNames[selected_class]}\")\n",
    "print(selected_class_data[0, :])\n",
    "\n",
    "print(f\"...and its projection onto PC2\")\n",
    "print(selected_class_data[0, :] @ V[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a plot the compare PCA on the standardized and non standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard deviations for each feature\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(np.arange(1, X.shape[1] + 1), np.std(X, axis=0))\n",
    "plt.xticks(np.arange(1, X.shape[1] + 1), attributeNames, rotation=90)\n",
    "plt.ylabel(\"Standard Deviation\")\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.title(\"Feature Standard Deviations\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Zero-mean data (not standardized)\n",
    "Y1 = X - np.mean(X, axis=0)\n",
    "\n",
    "# Standardized data (zero-mean, unit variance)\n",
    "Y2 = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# Store both versions\n",
    "Ys = [Y1, Y2]\n",
    "titles = [\"Zero-mean\", \"Zero-mean and unit variance\"]\n",
    "threshold = 0.9\n",
    "pc1, pc2 = 0, 1  # Principal components to plot\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "nrows, ncols = 3, 2  # Grid layout\n",
    "\n",
    "for k in range(2):\n",
    "    # PCA\n",
    "    U, S, Vh = svd(Ys[k], full_matrices=False)\n",
    "    V = Vh.T\n",
    "\n",
    "    if k == 1:\n",
    "        V = -V  # Flip to align with non-standardized PCA\n",
    "\n",
    "    # Compute variance explained\n",
    "    rho = (S ** 2) / np.sum(S ** 2)\n",
    "\n",
    "    # Compute projection onto principal components\n",
    "    Z = U * S\n",
    "\n",
    "    # Plot PCA projections\n",
    "    plt.subplot(nrows, ncols, 1 + k)\n",
    "    for c in np.unique(y):\n",
    "        plt.scatter(Z[y == c, pc1], Z[y == c, pc2], alpha=0.5, label=classNames[c])\n",
    "    plt.xlabel(f\"PC{pc1 + 1}\")\n",
    "    plt.ylabel(f\"PC{pc2 + 1}\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{titles[k]}: PCA Projection\")\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    # Plot attribute coefficients in PCA space\n",
    "    plt.subplot(nrows, ncols, 3 + k)\n",
    "    for att in range(V.shape[1]):\n",
    "        plt.arrow(0, 0, V[att, pc1], V[att, pc2])\n",
    "        plt.text(V[att, pc1], V[att, pc2], attributeNames[att])\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.xlabel(f\"PC{pc1 + 1}\")\n",
    "    plt.ylabel(f\"PC{pc2 + 1}\")\n",
    "    plt.grid()\n",
    "    plt.title(f\"{titles[k]}: Attribute Coefficients\")\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    # Plot cumulative variance explained\n",
    "    plt.subplot(nrows, ncols, 5 + k)\n",
    "    plt.plot(range(1, len(rho) + 1), rho, \"x-\", label=\"Individual Variance\")\n",
    "    plt.plot(range(1, len(rho) + 1), np.cumsum(rho), \"o-\", label=\"Cumulative Variance\")\n",
    "    plt.axhline(y=threshold, color=\"k\", linestyle=\"--\", label=\"90% Threshold\")\n",
    "    plt.title(f\"{titles[k]}: Variance Explained\")\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.ylabel(\"Variance Explained\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
